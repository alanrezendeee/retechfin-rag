O que Ã© RAG?

Maior benefÃ­cio do RAG Ã© CONTROLAR O RACIOCÃNIO DO MODELO.

RAG Ã© um Architectural Pattern, assim como MVC, CQRS, Event Sourcing.
ğŸ‘‰ RAG Ã© um padrÃ£o de recuperaÃ§Ã£o + raciocÃ­nio.

ğŸ¯ Por que o RAG existe?
Originalmente, para resolver 3 grandes limitaÃ§Ãµes dos LLMs:

LLMs nÃ£o sabem:
seus dados financeiros
seu CRM
seu ERP
seu sistema prisional
seus contratos

Sem RAG â†’ o modelo inventa.
Com RAG â†’ ele consulta sua fonte.

ğŸ‘‰ Isso muda TUDO.
VocÃª sai de:
"modelo opinativo"

para
"modelo baseado em evidÃªncia"

Isso Ã© arquitetura de sistema crÃ­tico.

Problemas que o RAG resolve:
Janela de contexto (tokens)
Imagine mandar:
10 mil despesas
contratos
logs
PDFs

A cada pergunta.
ğŸ‘‰ inviÃ¡vel.

RAG resolve com:
Retrieval seletivo
Busca sÃ³ o que importa.

ğŸ‘‰ 1ï¸âƒ£ DeterminÃ­sticas (matemÃ¡ticas / exatas)
ğŸ‘‰ 2ï¸âƒ£ SemÃ¢nticas (interpretaÃ§Ã£o / busca inteligente)
ğŸ‘‰ 2ï¸âƒ£ AnalÃ­ticas (insights / analise de dados)

O que aprendemos com este projeto?
1ï¸âƒ£ LLM nÃ£o Ã© calculadora
2ï¸âƒ£ RAG nÃ£o serve sÃ³ para "buscar texto"


Pilares do RAG:
1. Query Understanding - Query Parsing / Query Planning:
    Antes de buscar qualquer coisa, primeiro, vocÃª entende a pergunta do usuÃ¡rio.
    "Quanto paguei de aluguel no Ãºltimo trimestre?"
    O sistema transforma em:

    operation: sum
    categoria: aluguel
    periodo: Q4
    status: pago

Isso Ã© query engine

2. Retrieval inteligente:
    NÃ£o basta vetor.
    Hoje existem 3 camadas modernas:

    âœ”ï¸ Semantic search (vetor) - Traduzir infos da pergunta para as informaÃ§Ãµes que constam no banco de dados.
    "mac novo" â†’ "MacBook Pro 16"

    âœ”ï¸ Structured filtering - Dados financeiros nÃ£o podem ser tratados como documentos, sÃ£o dados estruturados.
    Vetores sozinhos sÃ£o ruins aqui.
    structured filter â†’ depois vetor

    âœ”ï¸ Hybrid Search (nÃ­vel alto)
    Combina:
    - filtro SQL-like
    - vetores
    - keyword
    - metadata

3. Tool Selection
    ğŸ‘‰ Nem toda pergunta deve ir para o LLM.
    "Qual o total?"
    Se mandar pro modelo:
        - ğŸ’¥ risco enorme.
        - LLM nÃ£o Ã© calculadora confiÃ¡vel.

    EntÃ£o fazemos:

    ğŸ‘‰ Tool Routing
    Decidimos:
        - vetor?
        - SQL?
        - cÃ¡lculo?
        - funÃ§Ã£o?
        - API?

4. Grounding
    ğŸ‘‰ obrigar o modelo a responder baseado em evidÃªncia.
    Seu prompt faz isso: "Use apenas as despesas abaixo para responder."
    Isso Ã© grounding.
    Sem isso:
        - modelo inventa.

5. Deterministic Layer
    Isso Ã© nÃ­vel arquitetura enterprise.
    Sempre que algo precisa ser:
        - somado
        - contado
        - ordenado
        - comparado

    ğŸ‘‰ NÃƒO USE O LLM.
    Use cÃ³digo.
    LLM apenas:
        - transforma o resultado em linguagem natural.
        - Isso reduz erro para quase zero.

ğŸ‘‰ Isso jÃ¡ Ã© um embriÃ£o de um AI Data Engine.
NÃ£o Ã© mais um chatbot.

VocÃª estÃ¡ criando um sistema que:
    - entende perguntas
    - decide estratÃ©gia
    - consulta dados
    - executa cÃ¡lculos
    - gera linguagem


ğŸ‘‰ RAG â†’ Tool Calling â†’ Agents â†’ AI Systems

âœ… Reduz alucinaÃ§Ã£o
âœ… Permite dados privados
âœ… Controla raciocÃ­nio
âœ… Escala conhecimento
âœ… Evita fine-tuning desnecessÃ¡rio
âœ… Cria sistemas auditÃ¡veis
âœ… Permite explainability

Explainability Ã© gigante.


Dicas:
ğŸ‘‰ NÃ£o transforme tudo em vetor.
Vetores NÃƒO sÃ£o banco universal.

Use vetor para:
    - linguagem
    - semÃ¢ntica
    - ambiguidade

Use estrutura para:
    - nÃºmeros
    - datas
    - filtros

Arquitetura hÃ­brida sempre vence.

Onde entra o LangChain?
LangChain Ã© uma biblioteca de engenharia de aplicaÃ§Ãµes com LLM.

Ele nÃ£o Ã© um modelo,
nÃ£o Ã© um banco vetorial,
nÃ£o Ã© uma AI.

ğŸ‘‰ Ele Ã© um orquestrador.

Pense assim:

OpenAI = motor
FAISS = Ã­ndice
Python = peÃ§as
LangChain = chassi que conecta tudo

Ele conecta:
    - LLM
    - vector DB
    - memÃ³ria
    - tools
    - agentes
    - prompts

sem vocÃª precisar escrever tanta infraestrutura.

1ï¸âƒ£ Criar RAG mais rÃ¡pido
    Sem LangChain vocÃª escreve:
        - embedding pipeline
        - search
        - prompt assembly
        - tool routing

2ï¸âƒ£ Criar Agents

Tipo:
    - "Se for cÃ¡lculo â†’ usa funÃ§Ã£o Python
    - Se for documento â†’ busca no vetor
    - Se for clima â†’ chama API"

LangChain jÃ¡ tem isso pronto.

3ï¸âƒ£ Tool Calling simplificado
    - tools = [
        - calculator,
        - sql_query,
        - rag_search
    ]
O agente decide sozinho qual usar.

4ï¸âƒ£ Padronizar arquitetura

Ele cria padrÃµes como:
ğŸ‘‰ Chains
ğŸ‘‰ Retrievers
ğŸ‘‰ Agents
ğŸ‘‰ Memory

Isso ajuda times grandes.

Ele sofre de um fenÃ´meno chamado:

ğŸ”´ â€œframework magicâ€
Funcionaâ€¦
atÃ© parar de funcionar.

AÃ­ vocÃª nÃ£o sabe:
    - onde estÃ¡ o bug
    - quem chamou quem
    - como otimizar
    - como reduzir custo

Porque ele abstrai demais.

ğŸ‘‰ O que arquitetos experientes fazem hoje?
ğŸ‘‰ LangChain-free architectures

Ou seja:
    - OpenAI SDK direto
    - FAISS direto
    - Python puro
    - tool routing manual

ğŸ‘‰ Quando EU recomendo LangChain?
âœ”ï¸ Prototipagem MUITO rÃ¡pida
âœ”ï¸ Hackathons
âœ”ï¸ MVP em dias
âœ”ï¸ Times juniores

Ele acelera.

ğŸ‘‰ Quando NÃƒO recomendo?

Se vocÃª quer:

âœ… controle de custo
âœ… previsibilidade
âœ… performance
âœ… debug fÃ¡cil
âœ… arquitetura limpa
âœ… evitar dependÃªncia


Arquitetura moderna de AI estÃ¡ indo para:
ğŸ‘‰ â€œThin orchestrationâ€

Ou seja:

Pouco framework.
Mais controle.

LangChain nÃ£o te faz sÃªnior.
Entender arquitetura faz.

NÃ£o use LangChain agora.

Continue:

ğŸ‘‰ OpenAI SDK
ğŸ‘‰ FAISS
ğŸ‘‰ pipelines prÃ³prios
ğŸ‘‰ deterministic layer

VocÃª estÃ¡ montando algo com cara de:

ğŸ‘‰ AI Data Platform

NÃ£o de chatbot.

_________________________________________________________

Vamos comeÃ§ar por organizar um pipeline determinÃ­stico e um pipeline semÃ¢ntico.
RaciocÃ­nio:
    O modelo precisa entender a pergunta e a resposta.
    "Quanto paguei de aluguel no Ãºltimo trimestre?"
    O sistema precisa entender:
    - a pergunta
    - a resposta
    - o contexto
_________________________________________________________


â­ PrÃ³xima melhoria (MUITO recomendada)

1. Query Planning
Hoje vocÃª ainda depende do LLM para detectar intenÃ§Ã£o.

Onde o modelo decide:
"isso Ã© cÃ¡lculo"
"isso Ã© busca"
"isso Ã© anÃ¡lise"


Falando em Persistir embeddings + FAISS, temos um problema que Ã© o custo de armazenamento.

ğŸ‘‰ Persistir embeddings + FAISS.
ConstruÃ§Ã£o de um motor financeiro AI

â­ Ordem ideal:
1ï¸âƒ£ Separar pipelines (acabamos de fazer)
2ï¸âƒ£ Persistir FAISS
3ï¸âƒ£ Criar handlers (total, mÃ©dia, maxâ€¦)
4ï¸âƒ£ Melhorar parser
5ï¸âƒ£ Criar agregaÃ§Ãµes financeiras
6ï¸âƒ£ Virar um agente


ğŸ‘‰ Como eliminar 80% das chamadas ao LLM


ğŸ”µ Perguntas SEMÃ‚NTICAS (interpretaÃ§Ã£o):
ğŸ‘‰ "Tive muitas despesas com cartÃ£o este ano?"
ğŸ‘‰ "Quais foram minhas maiores despesas?"
ğŸ‘‰ "Com quais empresas eu mais gasto?"
ğŸ‘‰ "Minhas despesas estÃ£o concentradas em qual categoria?"
ğŸ‘‰ "Existe algum gasto fora do padrÃ£o?"
ğŸ‘‰ "Quais despesas parecem recorrentes?"
ğŸ‘‰ "Tenho gastos com educaÃ§Ã£o?"
ğŸ‘‰ "O que estou pagando todo mÃªs?"
ğŸ‘‰ "Quais despesas sÃ£o pessoais e quais parecem empresariais?"
ğŸ‘‰ "Existe alguma despesa que parece desnecessÃ¡ria?"

ğŸŸ¢ Perguntas DETERMINÃSTICAS (matemÃ¡ticas)
ğŸ‘‰ "Qual o total gasto no ano?"
ğŸ‘‰ "Quanto gastei com cartÃ£o de crÃ©dito?"
ğŸ‘‰ "Qual foi o total pago?"
ğŸ‘‰ "Quanto ainda tenho pendente?"
ğŸ‘‰ "Qual minha maior despesa?"
ğŸ‘‰ "Qual minha menor despesa?"
ğŸ‘‰ "Quantas despesas jÃ¡ paguei?"
ğŸ‘‰ "Quantas estÃ£o em aberto?"
ğŸ‘‰ "Qual a mÃ©dia mensal de gastos?"
ğŸ‘‰ "Quanto gastei em janeiro?"

ğŸŸ£ Perguntas ANALÃTICAS
ğŸ‘‰ "Meu gasto estÃ¡ aumentando ao longo dos meses?"
ğŸ‘‰ "Qual tendÃªncia dos meus gastos?"
ğŸ‘‰ "Se eu continuar assim, quanto gastarei no ano?"
ğŸ‘‰ "Qual categoria cresceu mais?"
ğŸ‘‰ "Meu custo fixo mensal?"
ğŸ‘‰ "Quanto do meu gasto Ã© recorrente vs variÃ¡vel?"